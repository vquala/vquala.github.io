<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="style.css" />
</head>
<head>
  <meta charset="UTF-8">
  <title>VQualA: Visual Quality Assessment Competition</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>
  <header>
    <div class="container">
      <h1>Visual Quality Assessment Competition (VQualA)</h1>
      <p style="font-size: 2em;">ICCV 2025</p>
    </div>
</header>

  <nav>
    <div class="container">
      <ul>
        <li style="font-size: 22px;"><a href="#sponsors">Sponsors</a></li>
        <li style="font-size: 22px;"><a href="#cfp">Call for Papers</a></li>
        <li style="font-size: 22px;"><a href="#challenges">Challenges</a></li>
        <li style="font-size: 22px;"><a href="#speakers">Speakers</a></li>
        <li style="font-size: 22px;"><a href="#dates">Dates</a></li>
        <li style="font-size: 22px;"><a href="#organizers">Organizers</a></li>
      </ul>
    </div>
  </nav>
  <div class="container">
    
 <!-- Logo Image -->
<div class="logo-container">
  <img src="img/logo2.png" alt="VQualA Logo">
</div>


<section id="sponsors">
  <h2>Sponsors</h2>
  <div class="sponsor-container">
    <a href="https://research.snap.com/team/computational-imaging.html" target="_blank">
      <img src="img/WechatIMG1693.jpg" alt="Snap Inc.">
    </a>
    <a href="https://www.intsig.us/" target="_blank">
      <img src="img/9INTSIG1.jpg" alt="INTSIG">
    </a>
    <a href="https://talent.taotian.com/" target="_blank">
      <img src="img/WechatIMG2662.jpg" alt="alibaba">
    </a>
  </div>
</section>



  
<section id="cfp">
  <h2>Call for Papers</h2>
  <p>
    Visual quality assessment plays a crucial role in computer vision, serving as a fundamental step in tasks such as image quality assessment (IQA), image super-resolution, document image enhancement, and video restoration. Traditional visual quality assessment techniques often rely on scalar metrics like Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), which, while effective in certain contexts, fall short in capturing the perceptual quality experienced by human observers. This gap emphasizes the need for more perceptually aligned and comprehensive evaluation methods that can adapt to the growing demands of applications such as medical imaging, satellite remote sensing, immersive media, and document processing.

In recent years, advancements in deep learning, generative models, and multimodal large language models (MLLMs) have opened up new avenues for visual quality assessment. These models offer capabilities that extend beyond traditional scalar metrics, enabling more nuanced assessments through natural language explanations, open-ended visual comparisons, and enhanced context awareness. With these innovations, VQA is evolving to better reflect human perceptual judgments, making it a critical enabler for next-generation computer vision applications.
  <p>
    The VQualA Workshop aims to bring together researchers and practitioners from academia and industry to discuss and explore the latest trends, challenges, and innovations in visual quality assessment. We welcome original research contributions addressing, but not limited to, the following topics:

    <ul>
      <li>Image and video quality assessment</li>
      <li>Perceptual quality assessment techniques</li>
      <li>Multi-modal quality evaluation (image, video, text)</li>
      <li>Visual quality assessment for immersive media (VR/AR)</li>
      <li>Document image enhancement and quality analysis</li>
      <li>Quality assessment under adverse conditions (low light, weather distortions, motion blur)</li>
      <li>Robust quality metrics for medical and satellite imaging</li>
      <li>Perceptual-driven image and video super-resolution</li>
      <li>Visual quality in restoration tasks (denoising, deblurring, upsampling)</li>
      <li>Human-centric visual quality assessment</li>
      <li>Learning-based quality assessment models (CNNs, Transformers, MLLMs)</li>
      <li>Cross-domain visual quality adaptation</li>
      <li>Benchmarking and datasets for perceptual quality evaluation</li>
      <li>Integration of large language models for quality explanation and assessment</li>
      <li>Open-ended comparative assessments with natural language reasoning</li>
      <li>Emerging applications of VQA in autonomous driving, surveillance, and smart cities</li>
    </ul>
    <p>
      <strong>Submission Details</strong><br>
      Papers will be peer-reviewed and comply with the ICCV 2025 proceedings style, format and length. The camera-ready deadline aligns with the main conference. Accepted papers must be registered and presented to ensure their inclusion in the IEEE Xplore Library. 
      For details, refer to the <a href="https://iccv.thecvf.com/Conferences/2025/AuthorGuidelines" target="_blank" rel="noopener">ICCV 2025 Author Guidelines</a>.
    </p>
  
    <!-- Submission Button -->
    <div style="text-align: center; margin-top: 1.5em;">
      <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/VQualA"
         style="
           display: inline-block;
           background-color: #0066cc;
           color: #fff;
           text-decoration: none;
           font-weight: bold;
           padding: 0.75em 1.5em;
           border-radius: 999px;
           box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
           transition: background-color 0.3s ease, transform 0.2s ease;
         "
         onmouseover="this.style.backgroundColor='#005bb5'; this.style.transform='translateY(-2px)';"
         onmouseout="this.style.backgroundColor='#0066cc'; this.style.transform='translateY(0)';"
      >
        Submit You Paper
      </a>
    </div>
  </section>

  <section id="challenges">
    
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Challenges</title>
      <style>
          body {
              font-family: Arial, sans-serif;
              background-color: #f8f9fa;
              margin: 0;
              padding: 20px;
          }
          .challenge-list {
              list-style: none;
              padding: 0;
          }
          .challenge-item {
              background-color: #ffffff;
              margin-bottom: 15px;
              padding: 15px;
              border-radius: 8px;
              box-shadow: 0 2px 5px rgba(0, 0, 0, 0.15);
              transition: transform 0.2s;
          }
          .challenge-item:hover {
              transform: translateY(-3px);
          }
          .challenge-item a {
              text-decoration: none;
              color: #007bff;
              font-size: 1.2em;
              font-weight: bold;
          }
      </style>
  </head>
  <body>
  
  <h2>VQualA 2025 Challenges</h2>
  
  <ul class="challenge-list">
      <li class="challenge-item">
          <a href="https://codalab.lisn.upsaclay.fr/competitions/22924" target="_blank">ISRGC-Q: Image Super-Resolution Generated Content Quality Assessment Track</a>
      </li>
      <li class="challenge-item">
          <a href="https://codalab.lisn.upsaclay.fr/competitions/23017" target="_blank">FIQA: Face Image Quality Assessment Track </a>
      </li>
      <li class="challenge-item">
          <a href="https://codalab.lisn.upsaclay.fr/competitions/23005" target="_blank">EVQA-SnapUGC: Engagement Prediction for Short Videos Track </a>
      </li>
      <li class="challenge-item">
        <a href="https://codalab.lisn.upsaclay.fr/competitions/23016" target="_blank">Visual Quality Comparison for Large Multimodal Models Track</a>
    </li>
    <li class="challenge-item">
      <a href="https://codalab.lisn.upsaclay.fr/competitions/23020" target="_blank">DIQA: Document Image Quality Assessment Track</a>
  </li>
      <li class="challenge-item">
      <a href="https://codalab.lisn.upsaclay.fr/competitions/23067" target="_blank">GenAI-Bench AIGC Video Quality Assessment Challenge (Track I)</a>
  </li>
  </li>
      <li class="challenge-item">
      <a href="https://codalab.lisn.upsaclay.fr/competitions/23070" target="_blank">GenAI-Bench AIGC Video Quality Assessment Challenge (Track II)</a>
  </li>
  </ul>
  <h> One needs to check the corresponding competition page(s) in order to learn more about and to register to access the data and participate in the challenge(s) of interest.</h>
  
  </body>
</section>

    <section id="speakers">
      <h2>Confirmed Speakers</h2>
      <div class="speakers-container">
        <div class="speaker">
          <img src="img/alan-bovik.webp" alt="Alan Bovik">
          <div class="speaker-info">
            <h3><a href="https://www.ece.utexas.edu/people/faculty/alan-bovik" target="_blank">Prof. Alan Bovik</a></h3>
            <p>
              Prof. <strong><a href="https://www.ece.utexas.edu/people/faculty/alan-bovik" target="_blank">Alan Bovik</a></strong> (HonFRPS) holds the Cockrell Family Endowed Regents Chair in Engineering in the Chandra Family Department of Electrical and Computer Engineering in the Cockrell School of Engineering at The University of Texas at Austin, where he is Director of the Laboratory for Image and Video Engineering (LIVE). He is a faculty member in the Department of Electrical and Computer Engineering, the Wireless Networking and Communication Group, and the Institute for Neuroscience. His research interests include digital television, digital photography, visual perception, social media, and image and video processing. His work broadly focuses on creating new theories and algorithms that allow for the perceptually optimized streaming and sharing of visual media. The outcomes of his work have the benefits of ensuring the visual satisfaction of billions of viewers worldwide, while substantially reducing global bandwidth consumption. He has published over 1,000 technical articles in these areas. His publications have been cited more than 175,000 times in the literature, his H-index is above 135, and he is listed as a Highly-Cited Researcher by The Web of Science Group. His several books include the Handbook of Image and Video Processing (Academic Press, 2000, 2005), Modern Image Quality Assessment (2006), and the companion volumes The Essential Guides to Image and Video Processing (Academic Press, 2009).
            </p>
          </div>
        </div>
    
        <div class="speaker">
          <img src="img/balu-adsumilli.png" alt="Balu Adsumilli">
          <div class="speaker-info">
            <h3><a href="https://research.google/people/107756/?&type=google" target="_blank">Dr. Balu Adsumilli</a></h3>
            <p>
              Dr. <strong><a href="https://research.google/people/107756/?&type=google" target="_blank">Balu Adsumilli</a></strong> (IEEE Fellow) is the Head of Media Algorithms group at YouTube/Google, where he and his team research and develop algorithms to transform the uploaded videos to formats played across all your devices. Over the past years, he was instrumental in building and scaling technologies in the areas of video processing, computer vision, video compression, and video quality, which garnered Two Technology and Engineering Emmy awards for Google. Prior to YouTube, he was the Director of Advanced Technology at GoPro, where he led the Camera Architecture, and the Advanced Software teams, and developed their ProTune mode in collaboration with ACES and Technicolor. This paved the way for GoPro cameras capturing Industry neutral formats, and enabled their widespread applicability in the movie and television industry. Dr. Adsumilli serves on the board of the Television Academy, on the Visual Effects Society board, on the NATAS technical committee, on the IEEE Multimedia Signal Processing (MMSP) Technical Committee, the IEEE Image, Video, Multidimensional Signal Processing (IVMSP) Technical Committee, and on ACM Mile High Video Steering Committee. He has co-authored 125+ technical publications and holds 200+ US patents. He is on TPCs and organizing committees for various conferences and organized numerous workshops. He is a Fellow of IEEE, and an active member of ACM, SMPTE, VES, SPIE, and the Internet Society. He received his PhD from the University of California Santa Barbara, and masters from the University of Wisconsin Madison.
            </p>
          </div>
        </div>
      </div>


      <div class="speaker">
        <img src="img/WechatIMG2661.jpg" alt="Ying Chen">
        <div class="speaker-info">
          <h3><a href="https://scholar.google.com/citations?user=NpTmcKEAAAAJ&hl=en" target="_blank">Dr. Ying Chen</a></h3>
          <p>
            Dr. <strong><a href="https://scholar.google.com/citations?user=NpTmcKEAAAAJ&hl=en" target="_blank">Ying Chen</a></strong> (IEEE M’05 - SM’11) received a B.S. in Applied Mathematics and an M.S. in Electrical Engineering & Computer Science, both from Peking University, in 2001 and 2004, respectively. He received his PhD in Computing and Electrical Engineering from Tampere University of Technology (TUT), Finland, in 2010.
            Dr. Chen joined Alibaba Group in 2018 as a Senior Director. Before joining Alibaba, his earlier working experiences included Principal Engineer/Manager at Qualcomm Incorporated, San Diego, CA, USA, from 2009 to 2018; Researcher at TUT and Nokia Research Center, Finland, from 2006 to 2009; and Research Engineer at Thomson Corporate Research, Beijing, from 2004 to 2006.
            Dr. Chen is currently leading the Audiovisual Technology Group in Taobao, Alibaba, supporting end-to-end multimedia features and applications within Taobao. Dr. Chen has been focusing on multimedia algorithms. His group has won various winner awards at CVPR NTIRE, including the 2023 challenge on Quality Assessment of Video Enhancement, the 2022 challenge on Super-Resolution and Enhancement of Compressed Video, as well as the MSU Video Codecs Comparisons (2020, 2021, and 2023-2024).
            Dr. Chen contributed to three generations of video coding standards, including H.264/AVC, H.265/HEVC, and H.266/VVC, as well as video file format and transport standards. Dr. Chen has served as an editor and a software coordinator for H.264/AVC and H.265/HEVC (both for Multiview and 3D Video extensions). Dr. Chen has been serving as an Associate Editor for IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT).
            Dr. Chen’s research areas include video coding and transmission, image/video restoration and enhancement, image/video quality assessment, and generative AI for image/video. He has authored or co-authored about 90 academic papers and over 250 granted US patents in the above areas. His publications have been cited more than 20,000 times.
          </p>
        </div>
      </div>
    



    </section>
    
    <section id="dates">
      <h2>Important Dates</h2>
      <p>
        The participants’ submissions will be evaluated on the test set based on the metrics presented in the related paper for each respective task. The top 3 teams for each task will be invited to present their posters during the workshop.
      </p>
    
      <h3>Competition Timeline</h3>
      <ul>
        <li><strong>Competition starts:</strong> May 25, 2025</li>
        <li><strong>Submission deadline:</strong> June 25, 2025</li>
        <li><strong>Initial results release:</strong> June 28, 2025</li>
        <li><strong>Challenge paper deadline:</strong> July 7, 2025</li>
        <li><strong>Workshop paper deadline:</strong> June 25, 2025</li>
        <li><strong>Paper acceptance notification:</strong> July 11, 2025</li>
        <li><strong>Final results and winners announced:</strong> July 30, 2025</li>
      </ul>
    </section>
    
    
    
    <section id="organizers">
      <h2>List of Organizers</h2>
      <div class="organizers-grid">
    
        <div class="organizer">
          <img src="img/Wei Zhou.png" alt="Chris Zhou">
          <div class="org-info">
            <a href="https://weizhou-geek.github.io/" target="_blank" style="font-size: 1.2em;">
              <strong>Chris Wei Zhou</strong>
            </a><br>
            Cardiff University <br> UK<br>
            <a href="mailto:zhouw26@cardiff.ac.uk">zhouw26@cardiff.ac.uk</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Jian Wang.png" alt="Jian Wang">
          <div class="org-info">
            <a href="https://jianwang-cmu.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Jian Wang</strong>
            </a><br>
            Snap Research <br> USA<br>
            <a href="mailto:jwang4@snap.com">jwang4@snap.com</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Sishuo Ma.png" alt="Sizhuo Ma">
          <div class="org-info">
            <a href="https://research.snap.com/team_members/sizhuo-ma.html" target="_blank" style="font-size: 1.2em;">
            <strong>Sizhuo Ma</strong>
          </a><br>
            Snap Research <br> USA<br>
            <a href="mailto:sma@snap.com">sma@snap.com</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Xiongkuo Min.png" alt="Xiongkuo Min">
          <div class="org-info">
            <a href="https://minxiongkuo.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Xiongkuo Min</strong></a><br>
            Shanghai Jiao Tong University <br> China<br>
            <a href="mailto:minxiongkuo@sjtu.edu.cn">minxiongkuo@sjtu.edu.cn</a>
          </div>
        </div>
        <div class="organizer">
          <img src="img/WechatIMG4.jpg" alt="Xiaohong Liu">
          <div class="org-info">
            <a href="https://jhc.sjtu.edu.cn/~xiaohongliu/" target="_blank" style="font-size: 1.2em;">
            <strong>Xiaohong Liu</strong></a><br>
            Shanghai Jiao Tong University <br>
             China<br>
            <a href="mailto:xiaohongliu@sjtu.edu.cn">xiaohongliu@sjtu.edu.cn</a>
          </div>
        </div>

        <div class="organizer">
          <img src="img/Guangtao Zhai.png" alt="Guangtao Zhai">
          <div class="org-info">
            <a href="https://ee.sjtu.edu.cn/en/FacultyDetail.aspx?id=24&infoid=153&flag=153" target="_blank" style="font-size: 1.2em;">
            <strong>Guangtao Zhai</strong></a><br>
            Shanghai Jiao Tong University <br> China<br>
            <a href="mailto:zhaiguangtao@sjtu.edu.cn">zhaiguangtao@sjtu.edu.cn</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Zhengzhong Tu.png" alt="Zhengzhong Tu">
          <div class="org-info">
            <a href="https://vztu.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Zhengzhong Tu</strong></a><br>
            Texas A&amp;M University <br> USA<br>
            <a href="mailto:tzz@tamu.edu">tzz@tamu.edu</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Hadi Amirpour.png" alt="Hadi Amirpour">
          <div class="org-info">
            <a href="https://hadiamirpour.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Hadi Amirpour</strong></a><br>
            University of Klagenfurt <br> Austria<br>
            <a href="mailto:hadi.amirpour@aau.at">hadi.amirpour@aau.at</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Shiqi Wang.png" alt="Shiqi Wang">
          <div class="org-info">
            <a href="https://www.cs.cityu.edu.hk/~shiqwang/" target="_blank" style="font-size: 1.2em;">
            <strong>Shiqi Wang</strong></a><br>
            City University of Hong Kong<br> Hong Kong<br> 
            <a href="mailto:shiqwang@cityu.edu.hk">shiqwang@cityu.edu.hk</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Hanwei Zhu.png" alt="Hanwei Zhu">
          <div class="org-info">
            <a href="https://h4nwei.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Hanwei Zhu</strong></a><br>
            Nanyang Technological <br>University, Singapore<br>
            <a href="mailto:hanwei.zhu@ntu.edu.sg">hanwei.zhu@ntu.edu.sg</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Yixiao Li.png" alt="Yixiao Li">
          <div class="org-info">
            <a href="https://vquala.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Yixiao Li</strong></a><br>
            Cardiff University <br> UK<br>
            <a href="mailto:liy369@cardiff.ac.uk">liy369@cardiff.ac.uk</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Fan_Huang.png" alt="Fan Huang">
          <div class="org-info">
            <a href="https://vquala.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Fan Huang</strong></a><br>
            Shanghai Jiao Tong University<br> China<br>
            <a href="mailto:huangfan@sjtu.edu.cn">huangfan@sjtu.edu.cn</a>
          </div>
        </div>
    
        <div class="organizer">
          <img src="img/Shuo_Xing.png" alt="Shuo Xing">
          <div class="org-info">
            <a href="https://shuoxing98.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Shuo Xing</strong></a><br>
            Texas A&amp;M University <br> USA<br>
            <a href="mailto:shuoxing@tamu.edu">shuoxing@tamu.edu</a>
          </div>
        </div>

        <div class="organizer">
          <img src="img/Fengjun_Guo.png" alt="Fengjun Guo">
          <div class="org-info">
            <a href="https://vquala.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Fengjun Guo</strong></a><br>
            INTSIG Information Co. Ltd <br> China<br>
            <a href="mailto:fengjun_guo@intsig.net">fengjun_guo@intsig.net</a>
          </div>
        </div>

        <div class="organizer">
          <img src="img/xin.jpg" alt="Xin Li">
          <div class="org-info">
            <a href="https://lixinustc.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Xin Li</strong></a><br>
            University of Science and  <br> Technology  China<br>
            <a href="mailto:xin.li@ustc.edu.cn">xin.li@ustc.edu.cn</a>
          </div>
        </div>

        <div class="organizer">
          <img src="img/WechatIMG2569.jpg" alt="Wei-Ting Chen">
          <div class="org-info">
            <a href="https://sites.google.com/view/weitingchen/home" target="_blank" style="font-size: 1.2em;">
            <strong>Wei-Ting Chen</strong></a><br>
             Microsoft<br> USA <br>
            <a href="mailto:weitingchen@microsoft.com">weitingchen@microsoft.com</a>
          </div>
        </div>

        

        <div class="organizer">
          <img src="img/WechatIMG2765.jpg" alt="Xiaoshuai Hao">
          <div class="org-info">
            <a href="https://haoxiaoshuai.github.io/homepage/" target="_blank" style="font-size: 1.2em;">
            <strong>Xiaoshuai Hao</strong></a><br>
            Beijing Academy of  <br>
            Artificial Intelligence, China<br>
            <a href="mailto:xshao@baai.ac.cn">xshao@baai.ac.cn</a>
          </div>
        </div>

        <div class="organizer">
          <img src="img/WechatIMG3192.jpg" alt="Ying Chen">
          <div class="org-info">
            <a href="https://vquala.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Ying Chen</strong></a><br>
            TAOBAO & TMALL Group <br> China<br>

            <a href="mailto:yingchen@alibaba-inc.com">yingchen@alibaba-inc.com</a>
          </div>
        </div>

        <div class="organizer">
          <img src="img/WechatIMG3193.jpg" alt="Huasheng Wang">
          <div class="org-info">
            <a href="https://vquala.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Huasheng Wang</strong></a><br>
            TAOBAO & TMALL Group <br> China<br>

            <a href="mailto:lufei.whs@taobao.com">lufei.whs@taobao.com
            </a>
          </div>
        </div>

        <div class="organizer">
          <img src="img/WechatIMG3194.jpg" alt="Pengxiang Xiao">
          <div class="org-info">
            <a href="https://vquala.github.io/" target="_blank" style="font-size: 1.2em;">
            <strong>Pengxiang Xiao</strong></a><br>
            TAOBAO & TMALL Group <br> China<br>

            <a href="mailto:xiaopengxiang.xpx@taobao.com">xiaopengxiang.xpx@taobao.com
            </a>
          </div>
        </div>
    
      </div>
    </section>
    
    
    


  </div>
  <footer>
    <p>&copy; 2025 VQualA Workshop Organizers</p>
  </footer>
</body>
</html>
